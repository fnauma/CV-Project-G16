{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhP5VOJb0hMN"
      },
      "source": [
        "### Setup & Feature Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uAED3yaVs06"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbHb2Wsrl3h6"
      },
      "outputs": [],
      "source": [
        "!pip install open3d\n",
        "!pip install pillow pillow-heif tqdm\n",
        "!pip install exifread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21KgQ5GN1b2h"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import open3d as o3d\n",
        "\n",
        "from PIL import Image\n",
        "import pillow_heif\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import exifread\n",
        "\n",
        "import json\n",
        "\n",
        "from scipy.optimize import least_squares\n",
        "from scipy.sparse import lil_matrix\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc1shA4x3Gce"
      },
      "outputs": [],
      "source": [
        "#mounting drive to access images\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgjbGe_48I0t"
      },
      "source": [
        "Reading and Displaying Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKaq4TS141Kh"
      },
      "outputs": [],
      "source": [
        "jpg_images = \"/content/drive/MyDrive/cv/wall1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojNPHRDYSRxc"
      },
      "outputs": [],
      "source": [
        "#displaying the images\n",
        "\n",
        "def display_images(num_pics):\n",
        "    fig, axes = plt.subplots(1, num_pics, figsize=(15, 6))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i in range(1, num_pics + 1):\n",
        "        img_path = os.path.join(jpg_images, f\"{i}.jpg\")\n",
        "        if os.path.exists(img_path):\n",
        "            img = mpimg.imread(img_path)\n",
        "            ax = axes[i-1]\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"{i}.jpg\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_images(8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8CRyBW0UGdk"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqBHMj6vUl-4"
      },
      "outputs": [],
      "source": [
        "def preprocess_images(jpg_images, num_pics):\n",
        "    preprocessed = []\n",
        "\n",
        "    for i in range(1, num_pics + 1):\n",
        "        img_path = os.path.join(jpg_images, f\"{i}.jpg\")\n",
        "        if os.path.exists(img_path):\n",
        "            img = cv2.imread(img_path)\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            preprocessed.append(gray)\n",
        "\n",
        "    return preprocessed\n",
        "\n",
        "preprocessed = preprocess_images(jpg_images, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmHQ9y7xVpCn"
      },
      "source": [
        "ORB initiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j-f41tcVq4w"
      },
      "outputs": [],
      "source": [
        "# NEW: Initialize the SIFT detector\n",
        "sift = cv2.SIFT_create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45iFd4EV1xP"
      },
      "source": [
        "Running ORB model on first 5 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BewbD2x2V6Od"
      },
      "outputs": [],
      "source": [
        "def extract_features(preprocessed):\n",
        "    keypoints = []\n",
        "    descriptors = []\n",
        "\n",
        "    sift = cv2.SIFT_create()\n",
        "\n",
        "    for gray in preprocessed:\n",
        "        kp, des = sift.detectAndCompute(gray, None)\n",
        "        keypoints.append(kp)\n",
        "        descriptors.append(des)\n",
        "\n",
        "    return keypoints, descriptors\n",
        "\n",
        "def match_consecutive_images(descriptors):\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
        "    good_matches_list = []\n",
        "\n",
        "    for i in range(len(descriptors) - 1):\n",
        "        des1 = descriptors[i]\n",
        "        des2 = descriptors[i + 1]\n",
        "\n",
        "        matches = bf.knnMatch(des1, des2, k=2)\n",
        "\n",
        "        good_matches = []\n",
        "        for m, n in matches:\n",
        "            if m.distance < 0.75 * n.distance:  # Lowe's ratio test\n",
        "                good_matches.append(m)\n",
        "\n",
        "        good_matches_list.append(good_matches)\n",
        "        print(f\"Image {i+1} and Image {i+2}: {len(good_matches)} good matches\")\n",
        "\n",
        "    return good_matches_list\n",
        "\n",
        "\n",
        "keypoints, descriptors = extract_features(preprocessed)\n",
        "good_matches_list = match_consecutive_images(descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkLhPctbYzcJ"
      },
      "outputs": [],
      "source": [
        "#visualization of the feature matching\n",
        "\n",
        "def visualize_feature_matches(jpg_images, keypoints, good_matches_list, max_pairs=5):\n",
        "    for i, good_matches in enumerate(good_matches_list[:max_pairs]):\n",
        "        img1 = cv2.imread(os.path.join(jpg_images, f\"{i+1}.jpg\"), cv2.IMREAD_COLOR)\n",
        "        img2 = cv2.imread(os.path.join(jpg_images, f\"{i+2}.jpg\"), cv2.IMREAD_COLOR)\n",
        "\n",
        "        kp1 = keypoints[i]\n",
        "        kp2 = keypoints[i+1]\n",
        "\n",
        "        img_matches = cv2.drawMatches(\n",
        "            img1, kp1,\n",
        "            img2, kp2,\n",
        "            good_matches, None,\n",
        "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f'ORB Feature Matches: Image {i+1} and Image {i+2}')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "visualize_feature_matches(jpg_images, keypoints, good_matches_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkS2l6y2z9OB"
      },
      "source": [
        "### Two-View Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zq_DJuk02LK"
      },
      "source": [
        "Essential Matrix Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrmue-wI07yW"
      },
      "outputs": [],
      "source": [
        "def convert_keypoints_to_arrays(keypoints, good_matches_list):\n",
        "    all_pts1 = []\n",
        "    all_pts2 = []\n",
        "\n",
        "    for i in range(len(good_matches_list)):\n",
        "        kp1 = keypoints[i]\n",
        "        kp2 = keypoints[i + 1]\n",
        "        good_matches = good_matches_list[i]\n",
        "\n",
        "        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
        "        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
        "\n",
        "        all_pts1.append(pts1)\n",
        "        all_pts2.append(pts2)\n",
        "\n",
        "    return all_pts1, all_pts2\n",
        "\n",
        "def compute_intrinsic_matrix(jpg_images, sensor_width_mm=6.17, fallback_focal_mm=4.0):\n",
        "    img_path = os.path.join(jpg_images, \"1.jpg\")\n",
        "\n",
        "    with open(img_path, 'rb') as f:\n",
        "        tags = exifread.process_file(f)\n",
        "\n",
        "    if \"EXIF FocalLength\" in tags:\n",
        "        val = tags[\"EXIF FocalLength\"].values[0]\n",
        "        f_mm = float(val.num) / float(val.den)\n",
        "    else:\n",
        "        f_mm = fallback_focal_mm\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "    w, h = img.size\n",
        "\n",
        "    f_px = f_mm / sensor_width_mm * w\n",
        "    cx, cy = w / 2, h / 2\n",
        "\n",
        "    K = np.array([[f_px, 0, cx],\n",
        "                  [0,    f_px, cy],\n",
        "                  [0,       0,   1]])\n",
        "\n",
        "    return K\n",
        "\n",
        "import cv2\n",
        "\n",
        "def compute_essential_matrices(all_pts1, all_pts2, K):\n",
        "    essential_matrices = []\n",
        "    masks = []\n",
        "\n",
        "    for pts1, pts2 in zip(all_pts1, all_pts2):\n",
        "        E, mask = cv2.findEssentialMat(\n",
        "            pts1,\n",
        "            pts2,\n",
        "            K,\n",
        "            method=cv2.RANSAC,\n",
        "            prob=0.999,\n",
        "            threshold=1.0\n",
        "        )\n",
        "\n",
        "        essential_matrices.append(E)\n",
        "        masks.append(mask)\n",
        "\n",
        "    return essential_matrices, masks\n",
        "\n",
        "all_pts1, all_pts2 = convert_keypoints_to_arrays(keypoints, good_matches_list)\n",
        "K = compute_intrinsic_matrix(jpg_images)\n",
        "essential_matrices, masks = compute_essential_matrices(all_pts1, all_pts2, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYLA6cvp-ctr"
      },
      "source": [
        "Recovering Rotations R and Translations t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWK1AcZC_CER"
      },
      "outputs": [],
      "source": [
        "def recover_poses(all_pts1, all_pts2, masks, essential_matrices, K):\n",
        "    rotations = []\n",
        "    translations = []\n",
        "\n",
        "    for i in range(len(essential_matrices)):\n",
        "        pts1 = all_pts1[i]\n",
        "        pts2 = all_pts2[i]\n",
        "        E = essential_matrices[i]\n",
        "\n",
        "        # extract inlier points from RANSAC mask\n",
        "        pts1_in = pts1[masks[i].ravel() == 1]\n",
        "        pts2_in = pts2[masks[i].ravel() == 1]\n",
        "\n",
        "        # recover relative camera pose (R, t)\n",
        "        _, R, t, mask_pose = cv2.recoverPose(E, pts1_in, pts2_in, K)\n",
        "\n",
        "        rotations.append(R)\n",
        "        translations.append(t)\n",
        "\n",
        "    return rotations, translations\n",
        "\n",
        "rotations, translations = recover_poses(all_pts1, all_pts2, masks, essential_matrices, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxpMpRX-ETxa"
      },
      "source": [
        "Cheirality Check and Point Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG-MYjJPEaDw"
      },
      "outputs": [],
      "source": [
        "def triangulate_cheiralitycheck_ply(all_pts1, all_pts2, masks, essential_matrices, K, output_ply=\"point_cloud.ply\"):\n",
        "    # Use the first image pair\n",
        "    pts1 = all_pts1[0]\n",
        "    pts2 = all_pts2[0]\n",
        "\n",
        "    # extract RANSAC inliers\n",
        "    pts1_in = pts1[masks[0].ravel() == 1]\n",
        "    pts2_in = pts2[masks[0].ravel() == 1]\n",
        "\n",
        "    # Essential matrix for this pair\n",
        "    E = essential_matrices[0]\n",
        "\n",
        "    # decompose into 4 possible [R|t] poses\n",
        "    R1, R2, t = cv2.decomposeEssentialMat(E)\n",
        "    t = t.reshape(3, 1)\n",
        "\n",
        "    pose_candidates = [(R1, t), (R1, -t), (R2, t), (R2, -t)]\n",
        "\n",
        "    # Projection matrix for first camera\n",
        "    P1 = K @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
        "\n",
        "    best_pose = None\n",
        "    max_good_points = 0\n",
        "    best_pts3D = None\n",
        "\n",
        "    # evaluate each candidate pose\n",
        "    for R, t in pose_candidates:\n",
        "        P2 = K @ np.hstack((R, t))\n",
        "\n",
        "        pts4D_h = cv2.triangulatePoints(P1, P2, pts1_in.T, pts2_in.T)\n",
        "        pts3D = (pts4D_h[:3] / pts4D_h[3]).T\n",
        "\n",
        "        good_count = 0\n",
        "\n",
        "        for j in range(len(pts3D)):\n",
        "            if pts3D[j, 2] > 0:  # in front of camera 1\n",
        "                pt_cam2 = R @ pts3D[j] + t.ravel()\n",
        "                if pt_cam2[2] > 0:  # in front of camera 2\n",
        "                    good_count += 1\n",
        "\n",
        "        if good_count > max_good_points:\n",
        "            max_good_points = good_count\n",
        "            best_pose = (R, t)\n",
        "            best_pts3D = pts3D\n",
        "\n",
        "    R, t = best_pose\n",
        "    print(f\"Best pose has {max_good_points}/{len(best_pts3D)} cheirality-valid points\")\n",
        "\n",
        "    # Filter valid points\n",
        "    pts3D_filtered = []\n",
        "    for j in range(len(best_pts3D)):\n",
        "        if best_pts3D[j, 2] > 0:\n",
        "            pt_cam2 = R @ best_pts3D[j] + t.ravel()\n",
        "            if pt_cam2[2] > 0:\n",
        "                pts3D_filtered.append(best_pts3D[j])\n",
        "\n",
        "    pts3D_filtered = np.array(pts3D_filtered)\n",
        "\n",
        "    # Save PLY file\n",
        "    with open(output_ply, 'w') as f:\n",
        "        f.write('ply\\n')\n",
        "        f.write('format ascii 1.0\\n')\n",
        "        f.write(f'element vertex {len(pts3D_filtered)}\\n')\n",
        "        f.write('property float x\\n')\n",
        "        f.write('property float y\\n')\n",
        "        f.write('property float z\\n')\n",
        "        f.write('end_header\\n')\n",
        "        for pt in pts3D_filtered:\n",
        "            f.write(f'{pt[0]} {pt[1]} {pt[2]}\\n')\n",
        "\n",
        "    # visualize 3D point cloud\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(\n",
        "        pts3D_filtered[:, 0],\n",
        "        pts3D_filtered[:, 1],\n",
        "        pts3D_filtered[:, 2],\n",
        "        s=1\n",
        "    )\n",
        "    ax.set_title(\"Reconstructed 3D Point Cloud (Two-View)\")\n",
        "    plt.show()\n",
        "\n",
        "    return R, t, pts3D_filtered\n",
        "\n",
        "R, t, pts3D_filtered = triangulate_cheiralitycheck_ply(all_pts1, all_pts2, masks, essential_matrices, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5ZYjoBaDXYE"
      },
      "source": [
        "2D scatterplot of 3d points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06ge86YjD2E0"
      },
      "outputs": [],
      "source": [
        "#ignoring z axis and just focusing on the xy plane\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(pts3D_filtered[:, 0], pts3D_filtered[:, 1], s=1, alpha=0.6)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('2d scatter plot of 3d point cloud')\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFGlLhCh2I-"
      },
      "source": [
        "Incremental SFM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdfE_x3i9aRD"
      },
      "outputs": [],
      "source": [
        "def find_best_initial_pair():\n",
        "    best_pair_idx = 0\n",
        "    max_inliers = np.sum(masks[0].ravel() == 1)\n",
        "\n",
        "    for i in range(len(masks)):\n",
        "        num_inliers = np.sum(masks[i].ravel() == 1)\n",
        "        if num_inliers > max_inliers:\n",
        "            max_inliers = num_inliers\n",
        "            best_pair_idx = i\n",
        "\n",
        "    print(f\"✓ Using pair {best_pair_idx}-{best_pair_idx+1} with {max_inliers} inliers\")\n",
        "    return best_pair_idx\n",
        "\n",
        "def initialize_two_view(pair_idx):\n",
        "\n",
        "    pts1 = all_pts1[pair_idx]\n",
        "    pts2 = all_pts2[pair_idx]\n",
        "    E = essential_matrices[pair_idx]\n",
        "    mask = masks[pair_idx]\n",
        "\n",
        "    mask_essential = mask.ravel() == 1\n",
        "    pts1_in = pts1[mask_essential]\n",
        "    pts2_in = pts2[mask_essential]\n",
        "\n",
        "    R1, R2, t = cv2.decomposeEssentialMat(E)\n",
        "    t = t.reshape(3, 1)\n",
        "    pose_candidates = [(R1, t), (R1, -t), (R2, t), (R2, -t)]\n",
        "\n",
        "    P1 = K @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
        "\n",
        "    best_pose = None\n",
        "    max_good_points = 0\n",
        "    best_pts3D = None\n",
        "    best_mask = None\n",
        "\n",
        "    for R, t in pose_candidates:\n",
        "        P2 = K @ np.hstack((R, t))\n",
        "        pts4D_h = cv2.triangulatePoints(P1, P2, pts1_in.T, pts2_in.T)\n",
        "        pts3D = (pts4D_h[:3] / pts4D_h[3]).T\n",
        "\n",
        "        z1 = pts3D[:, 2]\n",
        "        z2 = (R @ pts3D.T + t).T[:, 2]\n",
        "        mask_cheirality = (z1 > 0) & (z2 > 0) & (z1 < 50) & (z2 < 50)\n",
        "        good_count = np.sum(mask_cheirality)\n",
        "\n",
        "        if good_count > max_good_points:\n",
        "            max_good_points = good_count\n",
        "            best_pose = (R, t)\n",
        "            best_pts3D = pts3D\n",
        "            best_mask = mask_cheirality\n",
        "\n",
        "    R, t = best_pose\n",
        "    pts3D_filtered = best_pts3D[best_mask]\n",
        "\n",
        "    essential_inlier_indices = np.where(mask_essential)[0]\n",
        "    cheirality_inlier_indices = essential_inlier_indices[best_mask]\n",
        "\n",
        "    keypoint_to_3d = defaultdict(dict)\n",
        "\n",
        "    for i, match_idx in enumerate(cheirality_inlier_indices):\n",
        "        match = good_matches_list[pair_idx][match_idx]\n",
        "        kp1_idx = match.queryIdx\n",
        "        kp2_idx = match.trainIdx\n",
        "\n",
        "        keypoint_to_3d[pair_idx][kp1_idx] = i\n",
        "        keypoint_to_3d[pair_idx + 1][kp2_idx] = i\n",
        "\n",
        "    print(f\"  Triangulated {len(pts3D_filtered)} points\")\n",
        "    print(f\"  Camera {pair_idx}: {len(keypoint_to_3d[pair_idx])} tracked\")\n",
        "    print(f\"  Camera {pair_idx + 1}: {len(keypoint_to_3d[pair_idx + 1])} tracked\")\n",
        "\n",
        "    return R, t, pts3D_filtered, keypoint_to_3d\n",
        "\n",
        "def initialize_global_state(best_pair_idx, R_init, t_init, pts3D_init, keypoint_to_3d):\n",
        "    camera_poses = {\n",
        "        best_pair_idx: (np.eye(3), np.zeros((3, 1))),\n",
        "        best_pair_idx + 1: (R_init, t_init)\n",
        "    }\n",
        "\n",
        "    points_3d = list(pts3D_init)\n",
        "    point_observations = [\n",
        "        [(best_pair_idx, kp_idx),\n",
        "         (best_pair_idx + 1, keypoint_to_3d[best_pair_idx + 1].get(kp_idx, -1))]\n",
        "        for kp_idx in keypoint_to_3d[best_pair_idx]\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nInitialized: {len(points_3d)} points, cameras {best_pair_idx} and {best_pair_idx + 1}\")\n",
        "\n",
        "    return camera_poses, points_3d, point_observations\n",
        "\n",
        "def add_next_camera(cam_idx, camera_poses, keypoint_to_3d, point_observations, points_3d):\n",
        "\n",
        "    print(f\"\\n--- Adding Camera {cam_idx} ---\")\n",
        "\n",
        "    prev_cam = None\n",
        "    matches = None\n",
        "    forward = True\n",
        "\n",
        "    if cam_idx - 1 in camera_poses:\n",
        "        prev_cam = cam_idx - 1\n",
        "        matches = good_matches_list[prev_cam]\n",
        "        forward = True\n",
        "    elif cam_idx + 1 in camera_poses:\n",
        "        prev_cam = cam_idx + 1\n",
        "        matches = good_matches_list[cam_idx]\n",
        "        forward = False\n",
        "    else:\n",
        "        print(\"No adjacent camera registered!\")\n",
        "        return False\n",
        "\n",
        "    print(f\"  Adjacent camera: {prev_cam} ({'forward' if forward else 'backward'})\")\n",
        "\n",
        "    pts_2d = []\n",
        "    pts_3d_indices = []\n",
        "\n",
        "    if forward:\n",
        "        for match in matches:\n",
        "            kp_prev_idx = match.queryIdx\n",
        "            kp_new_idx = match.trainIdx\n",
        "            if kp_prev_idx in keypoint_to_3d[prev_cam]:\n",
        "                pts_2d.append(keypoints[cam_idx][kp_new_idx].pt)\n",
        "                pts_3d_indices.append((keypoint_to_3d[prev_cam][kp_prev_idx], kp_new_idx))\n",
        "    else:\n",
        "        for match in matches:\n",
        "            kp_new_idx = match.queryIdx\n",
        "            kp_prev_idx = match.trainIdx\n",
        "            if kp_prev_idx in keypoint_to_3d[prev_cam]:\n",
        "                pts_2d.append(keypoints[cam_idx][kp_new_idx].pt)\n",
        "                pts_3d_indices.append((keypoint_to_3d[prev_cam][kp_prev_idx], kp_new_idx))\n",
        "\n",
        "    print(f\"  Found {len(pts_2d)} 2D-3D correspondences\")\n",
        "\n",
        "    if len(pts_2d) < 6:\n",
        "        print(f\"Not enough correspondences\")\n",
        "        return False\n",
        "\n",
        "    pts_2d = np.array(pts_2d, dtype=np.float32)\n",
        "    pts_3d_for_pnp = np.array([points_3d[i[0]] for i in pts_3d_indices], dtype=np.float32)\n",
        "\n",
        "    success, rvec, tvec, inliers = cv2.solvePnPRansac(\n",
        "        pts_3d_for_pnp, pts_2d, K, None,\n",
        "        iterationsCount=2000,\n",
        "        reprojectionError=4.0,\n",
        "        confidence=0.999\n",
        "    )\n",
        "\n",
        "    if not success or inliers is None or len(inliers) < 6:\n",
        "        print(f\"PnP failed (inliers: {len(inliers) if inliers is not None else 0})\")\n",
        "        return False\n",
        "\n",
        "    R_new, _ = cv2.Rodrigues(rvec)\n",
        "    t_new = tvec.reshape(3, 1)\n",
        "    camera_poses[cam_idx] = (R_new, t_new)\n",
        "\n",
        "    for inlier_idx in inliers.ravel():\n",
        "        pt3d_idx, kp_new_idx = pts_3d_indices[inlier_idx]\n",
        "        keypoint_to_3d[cam_idx][kp_new_idx] = pt3d_idx\n",
        "        point_observations[pt3d_idx].append((cam_idx, kp_new_idx))\n",
        "\n",
        "    print(f\"  ✓ Success: {len(inliers)} inliers, {len(keypoint_to_3d[cam_idx])} tracked\")\n",
        "\n",
        "    triangulate_with_camera(cam_idx, prev_cam, camera_poses, keypoint_to_3d, points_3d, point_observations)\n",
        "    return True\n",
        "\n",
        "def triangulate_with_camera(cam_idx, other_cam_idx, camera_poses, keypoint_to_3d, points_3d, point_observations):\n",
        "\n",
        "    if cam_idx > other_cam_idx:\n",
        "        matches = good_matches_list[other_cam_idx]\n",
        "        cam1, cam2 = other_cam_idx, cam_idx\n",
        "    else:\n",
        "        matches = good_matches_list[cam_idx]\n",
        "        cam1, cam2 = cam_idx, other_cam_idx\n",
        "\n",
        "    R1, t1 = camera_poses[cam1]\n",
        "    R2, t2 = camera_poses[cam2]\n",
        "\n",
        "    P1 = K @ np.hstack((R1, t1))\n",
        "    P2 = K @ np.hstack((R2, t2))\n",
        "\n",
        "    new_pts_1 = []\n",
        "    new_pts_2 = []\n",
        "    new_info = []\n",
        "\n",
        "    for match in matches:\n",
        "        kp1_idx = match.queryIdx\n",
        "        kp2_idx = match.trainIdx\n",
        "\n",
        "        if kp1_idx not in keypoint_to_3d[cam1] and kp2_idx not in keypoint_to_3d[cam2]:\n",
        "            new_pts_1.append(keypoints[cam1][kp1_idx].pt)\n",
        "            new_pts_2.append(keypoints[cam2][kp2_idx].pt)\n",
        "            new_info.append((kp1_idx, kp2_idx))\n",
        "\n",
        "    if len(new_pts_1) == 0:\n",
        "        return 0\n",
        "\n",
        "    new_pts_1 = np.array(new_pts_1).T\n",
        "    new_pts_2 = np.array(new_pts_2).T\n",
        "\n",
        "    pts4D = cv2.triangulatePoints(P1, P2, new_pts_1, new_pts_2)\n",
        "    pts3D_new = (pts4D[:3] / pts4D[3]).T\n",
        "\n",
        "    count_added = 0\n",
        "    for i, pt3d in enumerate(pts3D_new):\n",
        "        z1 = (R1 @ pt3d.reshape(3, 1) + t1)[2, 0]\n",
        "        z2 = (R2 @ pt3d.reshape(3, 1) + t2)[2, 0]\n",
        "\n",
        "        if 0 < z1 < 50 and 0 < z2 < 50:\n",
        "            idx = len(points_3d)\n",
        "            points_3d.append(pt3d)\n",
        "\n",
        "            kp1_idx, kp2_idx = new_info[i]\n",
        "            keypoint_to_3d[cam1][kp1_idx] = idx\n",
        "            keypoint_to_3d[cam2][kp2_idx] = idx\n",
        "            point_observations.append([(cam1, kp1_idx), (cam2, kp2_idx)])\n",
        "            count_added += 1\n",
        "\n",
        "    if count_added > 0:\n",
        "        print(f\"  Triangulated {count_added} new points\")\n",
        "\n",
        "    return count_added\n",
        "\n",
        "def run_full_reconstruction(best_pair_idx, camera_poses, points_3d, point_observations, keypoint_to_3d):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INCREMENTAL RECONSTRUCTION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    num_images = len(preprocessed)\n",
        "\n",
        "    cameras_forward = list(range(best_pair_idx + 2, num_images))\n",
        "    cameras_backward = list(range(best_pair_idx - 1, -1, -1))\n",
        "\n",
        "    print(f\"Forward: {cameras_forward}\")\n",
        "    print(f\"Backward: {cameras_backward}\")\n",
        "\n",
        "    for cam_idx in cameras_forward:\n",
        "        add_next_camera(cam_idx, camera_poses, keypoint_to_3d, point_observations, points_3d)\n",
        "\n",
        "    for cam_idx in cameras_backward:\n",
        "        add_next_camera(cam_idx, camera_poses, keypoint_to_3d, point_observations, points_3d)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Cameras registered: {len(camera_poses)}/{num_images}\")\n",
        "    print(f\"Total 3D points: {len(points_3d)}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "def visualize_and_save(points_3d, camera_poses):\n",
        "\n",
        "    points_3d_array = np.array(points_3d)\n",
        "    with open('point_cloud_multiview.ply', 'w') as f:\n",
        "        f.write('ply\\nformat ascii 1.0\\n')\n",
        "        f.write(f'element vertex {len(points_3d_array)}\\n')\n",
        "        f.write('property float x\\nproperty float y\\nproperty float z\\n')\n",
        "        f.write('end_header\\n')\n",
        "        for pt in points_3d_array:\n",
        "            f.write(f'{pt[0]} {pt[1]} {pt[2]}\\n')\n",
        "\n",
        "    print(\"\\n✓ Saved to 'point_cloud_multiview.ply'\")\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "    ax1 = fig.add_subplot(131, projection='3d')\n",
        "    ax1.scatter(points_3d_array[:, 0], points_3d_array[:, 1], points_3d_array[:, 2],\n",
        "                s=1, alpha=0.5)\n",
        "    ax1.set_title(f\"3D Points ({len(points_3d_array)})\")\n",
        "\n",
        "    ax2 = fig.add_subplot(132, projection='3d')\n",
        "    cam_positions = []\n",
        "    labels = []\n",
        "\n",
        "    for cam_idx in sorted(camera_poses):\n",
        "        R, t = camera_poses[cam_idx]\n",
        "        C = -R.T @ t\n",
        "        cam_positions.append(C.ravel())\n",
        "        labels.append(cam_idx)\n",
        "\n",
        "    cam_positions = np.array(cam_positions)\n",
        "    ax2.plot(cam_positions[:, 0], cam_positions[:, 1], cam_positions[:, 2], 'o-')\n",
        "    for i, lab in enumerate(labels):\n",
        "        pos = cam_positions[i]\n",
        "        ax2.text(pos[0], pos[1], pos[2], f'{lab}')\n",
        "\n",
        "    ax3 = fig.add_subplot(133, projection='3d')\n",
        "    ax3.scatter(points_3d_array[:, 0], points_3d_array[:, 1], points_3d_array[:, 2],\n",
        "                s=0.5, alpha=0.3)\n",
        "    ax3.plot(cam_positions[:, 0], cam_positions[:, 1], cam_positions[:, 2], 'o-')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "best_pair_idx = find_best_initial_pair()\n",
        "R_init, t_init, pts3D_init, keypoint_to_3d = initialize_two_view(best_pair_idx)\n",
        "camera_poses, points_3d, point_observations = initialize_global_state(best_pair_idx, R_init, t_init, pts3D_init, keypoint_to_3d)\n",
        "\n",
        "run_full_reconstruction(best_pair_idx, camera_poses, points_3d, point_observations, keypoint_to_3d)\n",
        "\n",
        "visualize_and_save(points_3d, camera_poses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipyvNYIch2I_"
      },
      "source": [
        "Refinement using Bundling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm6vnpuTJDz7"
      },
      "outputs": [],
      "source": [
        "def filter_outlier_points(points_3d, point_observations, camera_poses, keypoints, K, threshold=8.0):\n",
        "    \"\"\"\n",
        "    Remove 3D points with high reprojection errors before bundle adjustment\n",
        "    \"\"\"\n",
        "    print(f\"\\nFiltering outliers (threshold={threshold} pixels)...\")\n",
        "\n",
        "    good_points = []\n",
        "    good_observations = []\n",
        "\n",
        "    for pt_idx, pt3d in enumerate(points_3d):\n",
        "        observations = point_observations[pt_idx]\n",
        "        errors = []\n",
        "\n",
        "        for cam_idx, kp_idx in observations:\n",
        "            if cam_idx not in camera_poses:\n",
        "                continue\n",
        "\n",
        "            R, t = camera_poses[cam_idx]\n",
        "\n",
        "            # Project 3D point\n",
        "            pt3d_cam = R @ pt3d + t.flatten()\n",
        "            if pt3d_cam[2] <= 0:\n",
        "                continue\n",
        "\n",
        "            pt2d_h = K @ pt3d_cam\n",
        "            u_proj = pt2d_h[0] / pt2d_h[2]\n",
        "            v_proj = pt2d_h[1] / pt2d_h[2]\n",
        "\n",
        "            # Get observed keypoint\n",
        "            u_obs, v_obs = keypoints[cam_idx][kp_idx].pt\n",
        "\n",
        "            # Reprojection error\n",
        "            error = np.sqrt((u_proj - u_obs)**2 + (v_proj - v_obs)**2)\n",
        "            errors.append(error)\n",
        "\n",
        "        if len(errors) > 0:\n",
        "            mean_error = np.mean(errors)\n",
        "            if mean_error < threshold:\n",
        "                good_points.append(pt3d)\n",
        "                good_observations.append(observations)\n",
        "\n",
        "    print(f\"  Kept {len(good_points)}/{len(points_3d)} points\")\n",
        "    return good_points, good_observations\n",
        "\n",
        "\n",
        "def run_bundle_adjustment(camera_poses, points_3d, point_observations, K,\n",
        "                          max_nfev=50, verbose=2, xtol=1e-8, ftol=1e-8):\n",
        "    \"\"\"\n",
        "    Fixed bundle adjustment with conservative parameters\n",
        "    \"\"\"\n",
        "    cam_ids = sorted(camera_poses.keys())\n",
        "    num_cameras = len(cam_ids)\n",
        "    num_points = len(points_3d)\n",
        "\n",
        "    # Build observation arrays\n",
        "    obs_cam_idx, obs_point_idx, obs_xy = build_observation_arrays(\n",
        "        point_observations, keypoints, cam_ids\n",
        "    )\n",
        "    n_obs = len(obs_cam_idx)\n",
        "    print(f\"Observations: {n_obs}, cameras: {num_cameras}, points: {num_points}\")\n",
        "\n",
        "    # Pack initial parameters\n",
        "    x0 = pack_parameters(camera_poses, points_3d, cam_ids)\n",
        "\n",
        "    # Build jacobian sparsity pattern\n",
        "    J = build_jacobian_sparsity(num_cameras, num_points, obs_cam_idx, obs_point_idx)\n",
        "\n",
        "    def fun(params):\n",
        "        return residuals_vectorized(\n",
        "            params, K, num_cameras, num_points,\n",
        "            obs_cam_idx, obs_point_idx, obs_xy\n",
        "        )\n",
        "\n",
        "    # FIXED: More conservative parameters\n",
        "    res = least_squares(\n",
        "        fun,\n",
        "        x0,\n",
        "        jac_sparsity=J,\n",
        "        method='trf',\n",
        "        verbose=verbose,\n",
        "        max_nfev=max_nfev,  # Reduced iterations\n",
        "        x_scale='jac',      # Better scaling\n",
        "        loss='huber',       # Robust loss function\n",
        "        f_scale=1.0,        # Huber loss threshold\n",
        "        xtol=xtol,\n",
        "        ftol=ftol,\n",
        "    )\n",
        "\n",
        "    # Unpack and update camera poses & points\n",
        "    camera_params, point_params = unpack_parameters(res.x, num_cameras, num_points)\n",
        "\n",
        "    for i, cam_idx in enumerate(cam_ids):\n",
        "        rvec = camera_params[i][:3]\n",
        "        t = camera_params[i][3:].reshape(3, 1)\n",
        "        R, _ = cv2.Rodrigues(rvec)\n",
        "        camera_poses[cam_idx] = (R, t)\n",
        "\n",
        "    for i in range(num_points):\n",
        "        points_3d[i] = point_params[i]\n",
        "\n",
        "    print(\"✓ Bundle adjustment complete!\")\n",
        "    print(f\"  Final cost: {res.cost:.4f}\")\n",
        "    print(f\"  Optimality: {res.optimality:.2e}\")\n",
        "\n",
        "    return res\n",
        "\n",
        "def skew_mat(v):\n",
        "    \"\"\"v: (...,3) -> (...,3,3) skew-symmetric matrices\"\"\"\n",
        "    vx = v[..., 0]\n",
        "    vy = v[..., 1]\n",
        "    vz = v[..., 2]\n",
        "    zero = np.zeros_like(vx)\n",
        "    m = np.stack([\n",
        "        zero, -vz,  vy,\n",
        "         vz, zero, -vx,\n",
        "        -vy,  vx, zero\n",
        "    ], axis=-1)\n",
        "    return m.reshape(v.shape[:-1] + (3, 3))\n",
        "\n",
        "def rodrigues_batch(rvecs):\n",
        "    \"\"\"\n",
        "    rvecs: (N,3) axis-angle vectors\n",
        "    returns: R (N,3,3)\n",
        "    \"\"\"\n",
        "    theta = np.linalg.norm(rvecs, axis=1)\n",
        "    small = theta < 1e-8\n",
        "    R = np.zeros((len(rvecs), 3, 3), dtype=float)\n",
        "\n",
        "    # For non-small angles, use Rodrigues formula\n",
        "    nonzero_idx = ~small\n",
        "    if np.any(nonzero_idx):\n",
        "        r_non = rvecs[nonzero_idx]\n",
        "        theta_n = theta[nonzero_idx][:, None]\n",
        "        k = r_non / theta_n  # unit axis\n",
        "        K = skew_mat(k)      # (M,3,3)\n",
        "        I = np.eye(3)[None, :, :]\n",
        "        sin_t = np.sin(theta_n)[:, None]\n",
        "        cos_t = np.cos(theta_n)[:, None]\n",
        "        # R = I + sin(theta) * K + (1-cos(theta)) * K^2\n",
        "        R_n = I + sin_t * K + (1 - cos_t) * (K @ K)\n",
        "        R[nonzero_idx] = R_n\n",
        "\n",
        "    # For very small angles, use first-order approximation: R ≈ I + K\n",
        "    if np.any(small):\n",
        "        idx = np.where(small)[0]\n",
        "        K = skew_mat(rvecs[small])\n",
        "        I = np.eye(3)[None, :, :]\n",
        "        R_s = I + K\n",
        "        R[small] = R_s\n",
        "\n",
        "    return R  # (N,3,3)\n",
        "\n",
        "def build_observation_arrays(point_observations, keypoints, cam_ids):\n",
        "\n",
        "    cam_index = {cid: i for i, cid in enumerate(cam_ids)}\n",
        "    obs_cam_idx = []\n",
        "    obs_point_idx = []\n",
        "    obs_xy = []\n",
        "\n",
        "    for p_idx, obs in enumerate(point_observations):\n",
        "        for cam_idx, kp_idx in obs:\n",
        "            # skip missing kp (sometimes -1) just in case\n",
        "            if kp_idx is None or kp_idx < 0:\n",
        "                continue\n",
        "            obs_cam_idx.append(cam_index[cam_idx])\n",
        "            obs_point_idx.append(p_idx)\n",
        "            obs_xy.append(keypoints[cam_idx][kp_idx].pt)\n",
        "\n",
        "    obs_cam_idx = np.array(obs_cam_idx, dtype=np.int32)\n",
        "    obs_point_idx = np.array(obs_point_idx, dtype=np.int32)\n",
        "    obs_xy = np.array(obs_xy, dtype=np.float64)\n",
        "    return obs_cam_idx, obs_point_idx, obs_xy\n",
        "\n",
        "def pack_parameters(camera_poses, points_3d, cam_ids):\n",
        "    cams = []\n",
        "    for idx in cam_ids:\n",
        "        R, t = camera_poses[idx]\n",
        "        rvec, _ = cv2.Rodrigues(R)\n",
        "        cams.append(np.hstack([rvec.flatten(), t.flatten()]))\n",
        "    cams = np.hstack(cams)\n",
        "    pts = np.hstack(points_3d).flatten()\n",
        "    return np.hstack([cams, pts])\n",
        "\n",
        "def unpack_parameters(params, num_cameras, num_points):\n",
        "    camera_params = params[:num_cameras * 6].reshape((num_cameras, 6))\n",
        "    point_params = params[num_cameras * 6:].reshape((num_points, 3))\n",
        "    return camera_params, point_params\n",
        "\n",
        "def residuals_vectorized(params, K, num_cameras, num_points,\n",
        "                         obs_cam_idx, obs_point_idx, obs_xy):\n",
        "    \"\"\"\n",
        "    params: vector of length 6*num_cameras + 3*num_points\n",
        "    obs_*: arrays of length n_obs\n",
        "    returns: residuals shape (2*n_obs,)\n",
        "    \"\"\"\n",
        "    camera_params, point_params = unpack_parameters(params, num_cameras, num_points)\n",
        "    # camera params\n",
        "    rvecs = camera_params[:, :3]        # (C,3)\n",
        "    tvecs = camera_params[:, 3:].reshape(num_cameras, 3)  # (C,3)\n",
        "\n",
        "    # build rotation matrices\n",
        "    Rmats = rodrigues_batch(rvecs)  # (C,3,3)\n",
        "\n",
        "    # select per-observation camera R and t\n",
        "    R_obs = Rmats[obs_cam_idx]      # (n_obs,3,3)\n",
        "    t_obs = tvecs[obs_cam_idx]      # (n_obs,3)\n",
        "\n",
        "    # select 3D points per observation\n",
        "    P_obs = point_params[obs_point_idx]  # (n_obs,3)\n",
        "\n",
        "    # transform points into camera frame: X_c = R * X + t\n",
        "    Xc = np.einsum('nij,nj->ni', R_obs, P_obs) + t_obs  # (n_obs,3)\n",
        "\n",
        "    # perspective divide + intrinsics (assumes fx=K[0,0], fy=K[1,1], cx, cy)\n",
        "    X = Xc[:, 0]\n",
        "    Y = Xc[:, 1]\n",
        "    Z = Xc[:, 2]\n",
        "    # avoid divide by zero\n",
        "    Z_safe = np.where(Z == 0, 1e-8, Z)\n",
        "\n",
        "    fx = K[0, 0]\n",
        "    fy = K[1, 1]\n",
        "    cx = K[0, 2]\n",
        "    cy = K[1, 2]\n",
        "\n",
        "    u = fx * (X / Z_safe) + cx\n",
        "    v = fy * (Y / Z_safe) + cy\n",
        "\n",
        "    proj = np.vstack((u, v)).T  # (n_obs,2)\n",
        "    res = (proj - obs_xy).ravel()  # (2*n_obs,)\n",
        "    return res\n",
        "\n",
        "def build_jacobian_sparsity(num_cameras, num_points, obs_cam_idx, obs_point_idx):\n",
        "    n_obs = len(obs_cam_idx)\n",
        "    m = 2 * n_obs\n",
        "    n = 6 * num_cameras + 3 * num_points\n",
        "    J = lil_matrix((m, n), dtype=int)\n",
        "\n",
        "    for i in range(n_obs):\n",
        "        cam = obs_cam_idx[i]\n",
        "        p = obs_point_idx[i]\n",
        "        row_x = 2 * i\n",
        "        row_y = 2 * i + 1\n",
        "        # camera columns: 6 * cam  .. 6*cam+5\n",
        "        cam_col = 6 * cam\n",
        "        J[row_x, cam_col:cam_col + 6] = 1\n",
        "        J[row_y, cam_col:cam_col + 6] = 1\n",
        "        # point columns: 6*num_cameras + 3*p .. +2\n",
        "        pt_col = 6 * num_cameras + 3 * p\n",
        "        J[row_x, pt_col:pt_col + 3] = 1\n",
        "        J[row_y, pt_col:pt_col + 3] = 1\n",
        "\n",
        "    return J\n",
        "\n",
        "def run_bundle_adjustment(camera_poses, points_3d, point_observations, K,\n",
        "                               max_nfev=200, verbose=2, xtol=1e-10, ftol=1e-10):\n",
        "\n",
        "    cam_ids = sorted(camera_poses.keys())\n",
        "    num_cameras = len(cam_ids)\n",
        "    num_points = len(points_3d)\n",
        "\n",
        "    # Build observation arrays\n",
        "    obs_cam_idx, obs_point_idx, obs_xy = build_observation_arrays(point_observations, keypoints, cam_ids)\n",
        "    n_obs = len(obs_cam_idx)\n",
        "    print(f\"Observations: {n_obs}, cameras: {num_cameras}, points: {num_points}\")\n",
        "\n",
        "    # Pack initial parameters\n",
        "    x0 = pack_parameters(camera_poses, points_3d, cam_ids)\n",
        "\n",
        "    # Build jacobian sparsity pattern\n",
        "    J = build_jacobian_sparsity(num_cameras, num_points, obs_cam_idx, obs_point_idx)\n",
        "\n",
        "    # Closure wrapper for least_squares (to avoid passing many args repeatedly)\n",
        "    def fun(params):\n",
        "        return residuals_vectorized(params, K, num_cameras, num_points, obs_cam_idx, obs_point_idx, obs_xy)\n",
        "\n",
        "    # Run solver (TRF uses sparse Jacobian)\n",
        "    res = least_squares(\n",
        "        fun,\n",
        "        x0,\n",
        "        jac_sparsity=J,\n",
        "        method='trf',\n",
        "        verbose=verbose,\n",
        "        max_nfev=max_nfev,\n",
        "        xtol=xtol,\n",
        "        ftol=ftol,\n",
        "        # xtol_abs=1e-16\n",
        "    )\n",
        "\n",
        "    # Unpack and update camera poses & points\n",
        "    camera_params, point_params = unpack_parameters(res.x, num_cameras, num_points)\n",
        "\n",
        "    for i, cam_idx in enumerate(cam_ids):\n",
        "        rvec = camera_params[i][:3]\n",
        "        t = camera_params[i][3:].reshape(3, 1)\n",
        "        R, _ = cv2.Rodrigues(rvec)\n",
        "        camera_poses[cam_idx] = (R, t)\n",
        "\n",
        "    for i in range(num_points):\n",
        "        points_3d[i] = point_params[i]\n",
        "\n",
        "    print(\"✓ Bundle adjustment complete!\")\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AjOjI8KJI_x"
      },
      "outputs": [],
      "source": [
        "# Filter outliers before BA\n",
        "points_3d_filtered, point_observations_filtered = filter_outlier_points(\n",
        "    points_3d, point_observations, camera_poses, keypoints, K, threshold=8.0\n",
        ")\n",
        "\n",
        "# Update with filtered data\n",
        "points_3d[:] = points_3d_filtered\n",
        "point_observations[:] = point_observations_filtered\n",
        "\n",
        "# Run BA with conservative settings\n",
        "res = run_bundle_adjustment(\n",
        "    camera_poses,\n",
        "    points_3d,\n",
        "    point_observations,\n",
        "    K,\n",
        "    max_nfev=50,  # Reduced from 200\n",
        "    verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR1XanqNJZND"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_reconstruction(camera_poses, points_3d):\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.set_title(\"Bundle Adjustment Result (Cameras + 3D Points)\")\n",
        "\n",
        "    # Convert list of points to array\n",
        "    pts = np.array(points_3d)\n",
        "\n",
        "    # Plot 3D points\n",
        "    ax.scatter(pts[:,0], pts[:,1], pts[:,2], s=1, c=\"blue\", alpha=0.5)\n",
        "\n",
        "    # Plot cameras\n",
        "    for cam_id, (R, t) in camera_poses.items():\n",
        "        C = -R.T @ t   # camera center in world coords\n",
        "        C = C.reshape(3)\n",
        "\n",
        "        # Plot camera center\n",
        "        ax.scatter(C[0], C[1], C[2], c=\"red\", s=40)\n",
        "\n",
        "        # Plot camera orientation\n",
        "        z_dir = R.T @ np.array([0, 0, 1]) * 0.2\n",
        "        ax.plot(\n",
        "            [C[0], C[0] + z_dir[0]],\n",
        "            [C[1], C[1] + z_dir[1]],\n",
        "            [C[2], C[2] + z_dir[2]],\n",
        "            color=\"red\"\n",
        "        )\n",
        "\n",
        "        ax.text(C[0], C[1], C[2], f\"Cam {cam_id}\", color=\"red\")\n",
        "\n",
        "    ax.set_xlabel(\"X\")\n",
        "    ax.set_ylabel(\"Y\")\n",
        "    ax.set_zlabel(\"Z\")\n",
        "    ax.view_init(elev=15, azim=70)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_reconstruction(camera_poses, points_3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iox9BY4xBB4e"
      },
      "source": [
        "Exporting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxD3lPe6BMIV"
      },
      "outputs": [],
      "source": [
        "def export_cameras_to_json(camera_poses, keypoints, image_folder, output_file=\"cameras.json\"):\n",
        "\n",
        "    cameras_data = {\n",
        "        \"cameras\": []\n",
        "    }\n",
        "\n",
        "    for cam_idx in sorted(camera_poses.keys()):\n",
        "        R, t = camera_poses[cam_idx]\n",
        "\n",
        "        # Camera center in world coordinates: C = -R^T * t\n",
        "        C = -R.T @ t\n",
        "        C = C.flatten()\n",
        "\n",
        "        # Image filename\n",
        "        img_filename = f\"{cam_idx + 1}.jpg\"\n",
        "\n",
        "        camera_info = {\n",
        "            \"id\": int(cam_idx),\n",
        "            \"image\": img_filename,\n",
        "            \"rotation\": R.tolist(),\n",
        "            \"translation\": t.flatten().tolist(),\n",
        "            \"center\": C.tolist(),\n",
        "            \"num_keypoints\": len(keypoints[cam_idx])\n",
        "        }\n",
        "\n",
        "        cameras_data[\"cameras\"].append(camera_info)\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(cameras_data, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Exported {len(cameras_data['cameras'])} cameras to {output_file}\")\n",
        "    return cameras_data\n",
        "\n",
        "def create_view_graph(camera_poses, point_observations, threshold=10):\n",
        "    cam_ids = sorted(camera_poses.keys())\n",
        "    view_graph = {cam_id: [] for cam_id in cam_ids}\n",
        "\n",
        "    # Count shared points between camera pairs\n",
        "    shared_points = {}\n",
        "\n",
        "    for obs in point_observations:\n",
        "        cams_seeing_point = [cam_id for cam_id, kp_id in obs]\n",
        "\n",
        "        # Create edges between all cameras seeing this point\n",
        "        for i, cam1 in enumerate(cams_seeing_point):\n",
        "            for cam2 in cams_seeing_point[i+1:]:\n",
        "                if cam1 in cam_ids and cam2 in cam_ids:\n",
        "                    pair = tuple(sorted([cam1, cam2]))\n",
        "                    shared_points[pair] = shared_points.get(pair, 0) + 1\n",
        "\n",
        "    # Build adjacency list\n",
        "    for (cam1, cam2), count in shared_points.items():\n",
        "        if count >= threshold:\n",
        "            if cam2 not in view_graph[cam1]:\n",
        "                view_graph[cam1].append(cam2)\n",
        "            if cam1 not in view_graph[cam2]:\n",
        "                view_graph[cam2].append(cam1)\n",
        "\n",
        "    # Export view graph\n",
        "    view_graph_data = {\n",
        "        \"nodes\": [{\"id\": cam_id} for cam_id in cam_ids],\n",
        "        \"edges\": []\n",
        "    }\n",
        "\n",
        "    for cam1, neighbors in view_graph.items():\n",
        "        for cam2 in neighbors:\n",
        "            if cam1 < cam2:  # Avoid duplicates\n",
        "                view_graph_data[\"edges\"].append({\n",
        "                    \"source\": int(cam1),\n",
        "                    \"target\": int(cam2),\n",
        "                    \"weight\": int(shared_points.get(tuple(sorted([cam1, cam2])), 0))\n",
        "                })\n",
        "\n",
        "    with open(\"view_graph.json\", 'w') as f:\n",
        "        json.dump(view_graph_data, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Created view graph with {len(view_graph_data['edges'])} edges\")\n",
        "    return view_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sB50-PO4vvf"
      },
      "outputs": [],
      "source": [
        "export_cameras_to_json(camera_poses, keypoints, jpg_images, \"cameras.json\")\n",
        "view_graph = create_view_graph(camera_poses, point_observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR3dYtmp82Dg"
      },
      "outputs": [],
      "source": [
        "def extract_point_colors(points_3d, point_observations, camera_poses, keypoints, image_folder, K):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXTRACTING POINT COLORS FROM IMAGES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    colors = []\n",
        "\n",
        "    for pt_idx, pt3d in enumerate(points_3d):\n",
        "        observations = point_observations[pt_idx]\n",
        "        point_colors = []\n",
        "\n",
        "        # Sample colors from multiple views for robustness\n",
        "        for cam_idx, kp_idx in observations[:3]:  # Use up to 3 views\n",
        "            if cam_idx not in camera_poses:\n",
        "                continue\n",
        "\n",
        "            R, t = camera_poses[cam_idx]\n",
        "\n",
        "            # Project 3D point to image\n",
        "            pt3d_cam = R @ pt3d + t.flatten()\n",
        "\n",
        "            # Skip if behind camera\n",
        "            if pt3d_cam[2] <= 0:\n",
        "                continue\n",
        "\n",
        "            # Project to pixel coordinates\n",
        "            pt2d_h = K @ pt3d_cam\n",
        "            u = int(pt2d_h[0] / pt2d_h[2])\n",
        "            v = int(pt2d_h[1] / pt2d_h[2])\n",
        "\n",
        "            # Load image (cache this in practice)\n",
        "            img_path = os.path.join(image_folder, f\"{cam_idx + 1}.jpg\")\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            h, w = img.shape[:2]\n",
        "\n",
        "            # Check bounds\n",
        "            if 0 <= u < w and 0 <= v < h:\n",
        "                # Get color (OpenCV uses BGR, convert to RGB)\n",
        "                color_bgr = img[v, u]\n",
        "                color_rgb = color_bgr[[2, 1, 0]]  # BGR to RGB\n",
        "                point_colors.append(color_rgb)\n",
        "\n",
        "        # Average colors from multiple views\n",
        "        if len(point_colors) > 0:\n",
        "            avg_color = np.mean(point_colors, axis=0).astype(np.uint8)\n",
        "            colors.append(avg_color)\n",
        "        else:\n",
        "            # Default gray color if no valid projection\n",
        "            colors.append(np.array([128, 128, 128], dtype=np.uint8))\n",
        "\n",
        "        if (pt_idx + 1) % 1000 == 0:\n",
        "            print(f\"  Processed {pt_idx + 1}/{len(points_3d)} points...\")\n",
        "\n",
        "    colors = np.array(colors)\n",
        "    print(f\"✓ Extracted colors for {len(colors)} points\")\n",
        "    return colors\n",
        "\n",
        "def export_colored_ply(points_3d, colors, output_file=\"output/reconstruction_colored.ply\"):\n",
        "\n",
        "    points = np.array(points_3d)\n",
        "    colors = np.array(colors, dtype=np.uint8)\n",
        "\n",
        "    assert len(points) == len(colors), \"Points and colors must have same length\"\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        # Header\n",
        "        f.write('ply\\n')\n",
        "        f.write('format ascii 1.0\\n')\n",
        "        f.write(f'element vertex {len(points)}\\n')\n",
        "        f.write('property float x\\n')\n",
        "        f.write('property float y\\n')\n",
        "        f.write('property float z\\n')\n",
        "        f.write('property uchar red\\n')\n",
        "        f.write('property uchar green\\n')\n",
        "        f.write('property uchar blue\\n')\n",
        "        f.write('end_header\\n')\n",
        "\n",
        "        # Data\n",
        "        for pt, color in zip(points, colors):\n",
        "            f.write(f'{pt[0]} {pt[1]} {pt[2]} {color[0]} {color[1]} {color[2]}\\n')\n",
        "\n",
        "    print(f\"✓ Saved colored point cloud to {output_file}\")\n",
        "\n",
        "def add_colors_to_reconstruction(camera_poses, points_3d, point_observations,\n",
        "                                 keypoints, image_folder, K,\n",
        "                                 output_file=\"reconstruction_colored.ply\"):\n",
        "    # Extract colors\n",
        "    colors = extract_point_colors(\n",
        "        points_3d,\n",
        "        point_observations,\n",
        "        camera_poses,\n",
        "        keypoints,\n",
        "        image_folder,\n",
        "        K\n",
        "    )\n",
        "\n",
        "    # Export colored PLY\n",
        "    export_colored_ply(points_3d, colors, output_file)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ COLORED POINT CLOUD READY!\")\n",
        "    print(f\"  Use {output_file} in your viewer\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt_k8pMqBSvG"
      },
      "outputs": [],
      "source": [
        "# Add colors to your reconstruction\n",
        "colors = add_colors_to_reconstruction(\n",
        "    camera_poses,\n",
        "    points_3d,\n",
        "    point_observations,\n",
        "    keypoints,\n",
        "    jpg_images,\n",
        "    K,\n",
        "    \"wall_A.ply\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7vqMmG9AKZk"
      },
      "source": [
        "Running the whole pipeline for second wall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNCtW6FWAN0V"
      },
      "outputs": [],
      "source": [
        "jpg_images = \"/content/drive/MyDrive/cv/wall2\"\n",
        "display_images(12)\n",
        "preprocessed = preprocess_images(jpg_images, 12)\n",
        "sift = cv2.SIFT_create()\n",
        "keypoints, descriptors = extract_features(preprocessed)\n",
        "good_matches_list = match_consecutive_images(descriptors)\n",
        "all_pts1, all_pts2 = convert_keypoints_to_arrays(keypoints, good_matches_list)\n",
        "K = compute_intrinsic_matrix(jpg_images)\n",
        "essential_matrices, masks = compute_essential_matrices(all_pts1, all_pts2, K)\n",
        "rotations, translations = recover_poses(all_pts1, all_pts2, masks, essential_matrices, K)\n",
        "R, t, pts3D_filtered = triangulate_cheiralitycheck_ply(all_pts1, all_pts2, masks, essential_matrices, K)\n",
        "best_pair_idx = find_best_initial_pair()\n",
        "R_init, t_init, pts3D_init, keypoint_to_3d = initialize_two_view(best_pair_idx)\n",
        "camera_poses, points_3d, point_observations = initialize_global_state(best_pair_idx, R_init, t_init, pts3D_init, keypoint_to_3d)\n",
        "run_full_reconstruction(best_pair_idx, camera_poses, points_3d, point_observations, keypoint_to_3d)\n",
        "visualize_and_save(points_3d, camera_poses)\n",
        "\n",
        "# Filter outliers before BA\n",
        "points_3d_filtered, point_observations_filtered = filter_outlier_points(\n",
        "    points_3d, point_observations, camera_poses, keypoints, K, threshold=8.0\n",
        ")\n",
        "\n",
        "# Update with filtered data\n",
        "points_3d[:] = points_3d_filtered\n",
        "point_observations[:] = point_observations_filtered\n",
        "\n",
        "# Run BA with conservative settings\n",
        "res = run_bundle_adjustment(\n",
        "    camera_poses,\n",
        "    points_3d,\n",
        "    point_observations,\n",
        "    K,\n",
        "    max_nfev=50,  # Reduced from 200\n",
        "    verbose=2\n",
        ")\n",
        "plot_reconstruction(camera_poses, points_3d)\n",
        "export_cameras_to_json(camera_poses, keypoints, jpg_images, \"cameras_B.json\")\n",
        "view_graph = create_view_graph(camera_poses, point_observations)\n",
        "colors = add_colors_to_reconstruction(\n",
        "    camera_poses,\n",
        "    points_3d,\n",
        "    point_observations,\n",
        "    keypoints,\n",
        "    jpg_images,\n",
        "    K,\n",
        "    \"wall_B.ply\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTbi5wJzCBkW"
      },
      "source": [
        "Merging point clouds and combining cameras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_and_save():\n",
        "    pcd_A = o3d.io.read_point_cloud(\"wall_A.ply\")\n",
        "    pcd_B = o3d.io.read_point_cloud(\"wall_B.ply\")\n",
        "\n",
        "    # Scaling\n",
        "    bbox_A = pcd_A.get_axis_aligned_bounding_box()\n",
        "    bbox_B = pcd_B.get_axis_aligned_bounding_box()\n",
        "    height_A = bbox_A.get_max_bound()[2] - bbox_A.get_min_bound()[2]\n",
        "    height_B = bbox_B.get_max_bound()[2] - bbox_B.get_min_bound()[2]\n",
        "\n",
        "    scale_factor = height_A / height_B\n",
        "    pcd_B.scale(scale_factor, center=bbox_B.get_center())\n",
        "\n",
        "    rx = np.radians(5)\n",
        "    Rx = np.array([\n",
        "        [1, 0, 0],\n",
        "        [0, np.cos(rx), -np.sin(rx)],\n",
        "        [0, np.sin(rx),  np.cos(rx)]\n",
        "    ])\n",
        "\n",
        "    rz = np.radians(-5)\n",
        "    Rz = np.array([\n",
        "        [np.cos(rz), -np.sin(rz), 0],\n",
        "        [np.sin(rz),  np.cos(rz), 0],\n",
        "        [0,           0,          1]\n",
        "    ])\n",
        "\n",
        "    ry = np.radians(90)\n",
        "    Ry = np.array([\n",
        "        [np.cos(ry), 0, np.sin(ry)],\n",
        "        [0,          1, 0         ],\n",
        "        [-np.sin(ry),0, np.cos(ry)]\n",
        "    ])\n",
        "\n",
        "    # Combined rotation: Y then X then Z\n",
        "    R = Ry @ Rx @ Rz\n",
        "\n",
        "    t = np.array([10.0, 1.0, -12.0])\n",
        "\n",
        "    T = np.eye(4)\n",
        "    T[:3, :3] = R\n",
        "    T[:3, 3] = t\n",
        "\n",
        "    # Apply and save\n",
        "    pcd_B.transform(T)\n",
        "    combined = pcd_A + pcd_B\n",
        "\n",
        "    o3d.io.write_point_cloud(\"merged_room.ply\", combined, write_ascii=True)\n",
        "    np.save(\"transformation.npy\", T)\n",
        "\n",
        "    return T, scale_factor\n",
        "\n",
        "\n",
        "def transform_camera_pose(R_old, t_old, T):\n",
        "    R_transform = T[:3, :3]\n",
        "    t_transform = T[:3, 3].reshape(3, 1)\n",
        "\n",
        "    # Convert translation to camera center\n",
        "    t_old = np.array(t_old).reshape(3, 1)\n",
        "    C_old = -R_old.T @ t_old\n",
        "\n",
        "    # Apply transformation\n",
        "    R_new = R_transform @ R_old\n",
        "    C_new = R_transform @ C_old + t_transform\n",
        "\n",
        "    t_new = -R_new @ C_new\n",
        "    return R_new, t_new.flatten()\n",
        "\n",
        "\n",
        "def transform_cameras(T, scale_factor):\n",
        "\n",
        "    with open(\"cameras_B.json\", \"r\") as f:\n",
        "        cameras = json.load(f)\n",
        "\n",
        "    for cam in cameras[\"cameras\"]:\n",
        "        R_old = np.array(cam[\"rotation\"])\n",
        "        t_old = np.array(cam[\"translation\"])\n",
        "\n",
        "        # First scale the translation\n",
        "        t_old_scaled = t_old * scale_factor\n",
        "\n",
        "        # Then apply transformation\n",
        "        R_new, t_new = transform_camera_pose(R_old, t_old_scaled, T)\n",
        "\n",
        "        cam[\"rotation\"] = R_new.tolist()\n",
        "        cam[\"translation\"] = t_new.tolist()\n",
        "\n",
        "        # Update center as well\n",
        "        C_new = -R_new.T @ t_new.reshape(3, 1)\n",
        "        cam[\"center\"] = C_new.flatten().tolist()\n",
        "\n",
        "    with open(\"cameras_wall_B_transformed.json\", \"w\") as f:\n",
        "        json.dump(cameras, f, indent=2)\n",
        "\n",
        "\n",
        "def combine_cameras():\n",
        "\n",
        "    with open(\"cameras.json\", \"r\") as f:\n",
        "        cameras_A = json.load(f)\n",
        "\n",
        "    with open(\"cameras_wall_B_transformed.json\", \"r\") as f:\n",
        "        cameras_B = json.load(f)\n",
        "\n",
        "    all_cameras = {\n",
        "        \"cameras\": cameras_A[\"cameras\"] + cameras_B[\"cameras\"]\n",
        "    }\n",
        "\n",
        "    with open(\"all_cameras.json\", \"w\") as f:\n",
        "        json.dump(all_cameras, f, indent=2)\n",
        "\n",
        "\n",
        "# Main execution\n",
        "print(\"Step 1: Merging point clouds...\")\n",
        "T, scale_factor = merge_and_save()\n",
        "\n",
        "print(\"\\nStep 2: Transforming cameras...\")\n",
        "transform_cameras(T, scale_factor)\n",
        "\n",
        "print(\"\\nStep 3: Combining all cameras...\")\n",
        "combine_cameras()\n",
        "\n",
        "print(\"\\n✓ Complete! Use merged_room.ply and all_cameras.json\")"
      ],
      "metadata": {
        "id": "3rpJqMrH6Cm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}